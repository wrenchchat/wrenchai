# Bayes' Theorem

## 1. Core Concepts: Understanding Bayes' Theorem

Bayes' Theorem, in its simplest form, states:

```
P(A|B) = [P(B|A) * P(A)] / P(B)
```

Where:

+ P(A|B): The *posterior probability* of event A occurring given that event B has occurred. This is what we want to calculate.  In your agent context, this could be the probability of a decision being correct *given* the evidence gathered.
+ P(B|A): The *likelihood* of event B occurring given that event A has occurred. This is the probability of observing the evidence *if* the decision were correct.
+ P(A): The *prior probability* of event A occurring. This is our initial belief about the decision's correctness *before* considering the evidence.
+ P(B): The *evidence probability* (or marginal likelihood) of event B occurring. This acts as a normalizing constant and can sometimes be tricky to calculate directly.

## 2.  Discrete vs. Continuous Events

+ Discrete Events: Events with a finite (or countably infinite) number of outcomes. Example: "The agent chooses action X" (where X is one of a set of possible actions).  We'll focus on discrete events first, as they're easier to implement.
+ Continuous Events: Events with an infinite number of possible outcomes within a range. Example: "The confidence score of the LLM is 0.85". We'll touch on how to handle these later.

## 3. Python Implementation (Discrete Case)

Let's start with a simple, concrete example to make the implementation clear. Imagine an agent deciding whether to use a "Summarization" tool or a "Question Answering" tool based on user input.

```python
def bayes_theorem(prior_prob_A, likelihood_B_given_A, evidence_probs):
    """
    Calculates the posterior probability using Bayes' Theorem (discrete case).

    Args:
        prior_prob_A: dict, Prior probabilities of each event A. 
                       Example: {'Summarization': 0.6, 'QA': 0.4}
        likelihood_B_given_A: dict of dicts, Likelihood of evidence B given event A.
                              Example: {'Summarization': {'long_input': 0.8, 'short_input': 0.2},
                                        'QA': {'long_input': 0.3, 'short_input': 0.7}}
        evidence_probs: dict, Probability of each evidence B. Optional, can be calculated.
                              Example: {'long_input': 0.55, 'short_input': 0.45}

    Returns:
        dict: Posterior probabilities of each event A given the observed evidence B.
    """

    observed_evidence = list(evidence_probs.keys())[0]  #  Assume only ONE piece of evidence is observed at a time.

    posterior_probs = {}
    for event_A in prior_prob_A:
        numerator = likelihood_B_given_A[event_A][observed_evidence] * prior_prob_A[event_A]
        denominator = evidence_probs[observed_evidence] #Use pre-calculated evidence probability
        posterior_probs[event_A] = numerator / denominator

    return posterior_probs


def calculate_evidence_probability(prior_prob_A, likelihood_B_given_A, observed_evidence):
    """
    Calculates P(B), the probability of the evidence.
    """
    p_b = 0
    for event_A in prior_prob_A:
        p_b += likelihood_B_given_A[event_A][observed_evidence] * prior_prob_A[event_A]
    return p_b


# --- Example Usage ---

# Prior probabilities of choosing each tool
prior_probabilities = {
    'Summarization': 0.6,  # We initially believe Summarization is more likely
    'QA': 0.4
}

# Likelihoods of observing input length given the tool choice
likelihoods = {
    'Summarization': {
        'long_input': 0.8,  # High likelihood of long input if Summarization is correct
        'short_input': 0.2
    },
    'QA': {
        'long_input': 0.3,  # Lower likelihood of long input if QA is correct
        'short_input': 0.7
    }
}

# --- Scenario 1: Observe long input ---
observed_evidence1 = 'long_input'
evidence_probability1 = calculate_evidence_probability(prior_probabilities, likelihoods, observed_evidence1)
# or, calculate evidence probability within the bayes_theorem function

posterior_probs1 = bayes_theorem(prior_probabilities, likelihoods, {'long_input': evidence_probability1})
print(f"Scenario 1 (Long Input): {posterior_probs1}")
# Expected Output (approximately):  {'Summarization': 0.842, 'QA': 0.158}


# --- Scenario 2: Observe short input ---
observed_evidence2 = 'short_input'
evidence_probability2 = calculate_evidence_probability(prior_probabilities, likelihoods, observed_evidence2)
# or, calculate evidence probability within the bayes_theorem function

posterior_probs2 = bayes_theorem(prior_probabilities, likelihoods, {'short_input': evidence_probability2})
print(f"Scenario 2 (Short Input): {posterior_probs2}")
# Expected Output (approximately): {'Summarization': 0.31, 'QA': 0.69}

```

Key improvements and explanations in this code:

+ Clearer Function Structure:  Separate functions for `bayes_theorem` and `calculate_evidence_probability` improve readability and reusability.
+ Dictionaries for Probabilities: Using dictionaries to represent probabilities is crucial for handling multiple events and evidence types.  This structure makes the code scalable.
+ Evidence Probability Calculation: The `calculate_evidence_probability` function implements the law of total probability to compute `P(B)`:  `P(B) = Î£ [P(B|A) * P(A)]` for all possible A. This is *essential* for correct Bayesian inference.  The example shows two ways to handle this: pre-calculating it or calculating it within the main function.
+ Handling Single Evidence: The line `observed_evidence = list(evidence_probs.keys())[0]` assumes that only *one* piece of evidence is observed at a time. This simplifies the logic for this example, but we'll address multiple pieces of evidence later.
+ Example Scenarios: The code includes two scenarios ("long input" and "short input") to demonstrate how the posterior probabilities change based on the observed evidence. The expected outputs show how the agent's belief shifts.
* Docstrings: Thorough explanation of parameters

## 4. Handling Multiple Pieces of Evidence (Naive Bayes)

If your agent receives multiple pieces of evidence (e.g., input length *and* number of keywords), you can extend the above approach using the *Naive Bayes* assumption.  This assumption states that the pieces of evidence are conditionally independent given the event A.  This is often *not* strictly true in real-world scenarios, but it simplifies the calculations considerably and often works surprisingly well.

```python
def naive_bayes(prior_prob_A, likelihood_B_given_A, observed_evidences):
    """
    Calculates posterior probabilities using Naive Bayes.

    Args:
        prior_prob_A: dict, Prior probabilities of each event A.
        likelihood_B_given_A: dict of dicts, Likelihood of evidence B given event A.
        observed_evidences: list, List of observed evidence.  e.g., ['long_input', 'many_keywords']
    """
    posterior_probs = {}

    for event_A in prior_prob_A:
        numerator = prior_prob_A[event_A]
        for evidence in observed_evidences:
            numerator *= likelihood_B_given_A[event_A].get(evidence, 1)  # Handle missing likelihoods

        denominator = 0
        for event_Ai in prior_prob_A:
            temp_denom = prior_prob_A[event_Ai]
            for evidence in observed_evidences:
                temp_denom *= likelihood_B_given_A[event_Ai].get(evidence, 1)
            denominator += temp_denom
        
        if denominator == 0:
            posterior_probs[event_A] = 0  # Avoid division by zero
        else:
            posterior_probs[event_A] = numerator / denominator

    return posterior_probs

# --- Example with Multiple Evidence ---

# Expanded likelihoods to include keyword count
likelihoods = {
    'Summarization': {
        'long_input': 0.8,
        'short_input': 0.2,
        'many_keywords': 0.4,
        'few_keywords': 0.6
    },
    'QA': {
        'long_input': 0.3,
        'short_input': 0.7,
        'many_keywords': 0.7,
        'few_keywords': 0.3
    }
}

observed_evidences = ['long_input', 'many_keywords']
posterior_probs_multiple = naive_bayes(prior_probabilities, likelihoods, observed_evidences)
print(f"Multiple Evidence: {posterior_probs_multiple}")
# Expected Output (approximately): {'Summarization': 0.7, 'QA': 0.3}
```

Key changes for Naive Bayes:

+ Iterating Through Evidence: The code now iterates through the `observed_evidences` list, multiplying the likelihoods for each piece of evidence.
+ Handling Missing Likelihoods: The `.get(evidence, 1)` part is crucial. If a particular evidence type is *not* defined in the `likelihoods` for a given event A, it defaults to a likelihood of 1.  This prevents the entire numerator from becoming zero due to a single missing likelihood.  You might choose a different default (e.g., a small non-zero value) depending on your application.
+ Denominator Calculation (Law of Total Probability):  The denominator is calculated by summing the numerators across all possible events (A), ensuring proper normalization.
+ Zero Denominator Handling: The code includes a check for a zero denominator to prevent division-by-zero errors. This can happen if the observed evidence is extremely unlikely under all possible events.

## 5. Handling Continuous Variables (Gaussian Naive Bayes)

If you have continuous evidence (like a confidence score), you can use Gaussian Naive Bayes.  This assumes that the likelihood of a continuous variable, given an event, follows a Gaussian (normal) distribution.

```python
import numpy as np
from scipy.stats import norm

def gaussian_naive_bayes(prior_prob_A, means, std_devs, observed_values):
    """
    Calculates posterior probabilities using Gaussian Naive Bayes.

    Args:
        prior_prob_A: dict, Prior probabilities of each event A.
        means: dict of dicts, Mean of each continuous variable given event A.
        std_devs: dict of dicts, Standard deviation of each variable given A.
        observed_values: dict, Observed values of continuous variables.
                           e.g., {'confidence': 0.85, 'response_time': 2.5}
    """
    posterior_probs = {}

    for event_A in prior_prob_A:
        numerator = prior_prob_A[event_A]
        for variable, value in observed_values.items():
            mean = means[event_A][variable]
            std_dev = std_devs[event_A][variable]
            likelihood = norm.pdf(value, loc=mean, scale=std_dev)  # Gaussian PDF
            numerator *= likelihood

        denominator = 0
        for event_Ai in prior_prob_A:
            temp_denom = prior_prob_A[event_Ai]
            for variable, value in observed_values.items():
                mean = means[event_Ai][variable]
                std_dev = std_devs[event_Ai][variable]
                likelihood = norm.pdf(value, loc=mean, scale=std_dev)
                temp_denom *= likelihood
            denominator += temp_denom

        if denominator == 0:
            posterior_probs[event_A] = 0
        else:
            posterior_probs[event_A] = numerator / denominator
    return posterior_probs


# --- Example with Continuous Variables ---
prior_probabilities = {
    'LLM_A': 0.5,
    'LLM_B': 0.5
}

# Means and standard deviations for confidence and response time
means = {
    'LLM_A': {'confidence': 0.9, 'response_time': 1.0},
    'LLM_B': {'confidence': 0.7, 'response_time': 3.0}
}
std_devs = {
    'LLM_A': {'confidence': 0.05, 'response_time': 0.2},
    'LLM_B': {'confidence': 0.1, 'response_time': 0.5}
}

observed_values = {'confidence': 0.88, 'response_time': 1.2}
posterior_probs_continuous = gaussian_naive_bayes(prior_probabilities, means, std_devs, observed_values)
print(f"Continuous Evidence: {posterior_probs_continuous}")
# Expected output (approximately): {'LLM_A': 0.976, 'LLM_B': 0.024}

```

Key changes for Gaussian Naive Bayes:

+ `scipy.stats.norm`:  We use the `norm.pdf` function from `scipy.stats` to calculate the probability density of the observed value given the mean and standard deviation for each variable and event.
+ Means and Standard Deviations:  Instead of likelihood dictionaries, we now have dictionaries for `means` and `std_devs`, representing the parameters of the Gaussian distributions.
+ Observed Values as Dictionary: The `observed_values` are now passed as a dictionary, mapping variable names to their observed values.

## 6.  Integrating with Your Agentic Framework

Here's how you can integrate these Bayesian calculations into your agentic framework:

+ Reflection and Reasoning: After an agent performs an action (e.g., uses a tool, generates code), it can gather evidence about the outcome.  This evidence is then used to update the agent's belief about the effectiveness of that action using Bayes' Theorem.
+ Decision Evaluation: Before making a decision, an agent can use its current beliefs (posterior probabilities from previous interactions) as priors.  It can then simulate potential outcomes and their likelihoods, and use Bayes' Theorem to estimate the posterior probability of success for each possible decision.
+ LLM Selection: The Gaussian Naive Bayes example demonstrates how you can use continuous metrics (confidence, response time) to choose the best LLM for a task.  The posterior probabilities represent the agent's belief about which LLM is most likely to be successful given the observed metrics.
+ Dynamic Priors: The posterior probabilities from one step become the prior probabilities for the next step.  This allows the agent to learn and adapt over time.
+ Cache-Augmented Generation: The cache can store past observations and their associated probabilities.  This can significantly speed up the Bayesian calculations, especially if the agent encounters similar situations repeatedly.
* Playbooks, Handoff, and QC: The conditional probability calculations are part of the QC mechanism. If P(good result | action, context) falls below a threshold defined in the playbook, a handoff might be triggered.

## 7.  Important Considerations and Next Steps

+ Defining Priors: Choosing appropriate prior probabilities can be challenging. You might start with uniform priors (equal probabilities for all events) if you have no prior knowledge.  Alternatively, you could use domain expertise or data from previous interactions to inform the priors.
+ Estimating Likelihoods:  Accurately estimating the likelihoods (`P(B|A)`) is critical. You can use a combination of:
    + Empirical Data: Collect data from the agent's interactions and use it to estimate the likelihoods.
    + Expert Knowledge: Encode domain expertise into the likelihoods.
    + LLM Self-Assessment: Prompt the LLM to estimate its own confidence or the likelihood of success.
+ Computational Cost: Bayesian calculations can become computationally expensive, especially with many events and evidence types.  Consider using techniques like:
    + Approximation Methods: For complex scenarios, you might need to use approximate inference methods like Markov Chain Monte Carlo (MCMC) or Variational Inference.
    + Caching:  Store intermediate results to avoid redundant calculations.
+ Beyond Naive Bayes: If the independence assumption of Naive Bayes is too strong, you might need to explore more complex Bayesian models, such as Bayesian Networks. These models can represent dependencies between variables, but they are also more complex to implement.
+ Online Learning: Implement mechanisms for the agent to continuously update its priors and likelihoods as it gathers more data. This can be done using techniques like Bayesian updating or online learning algorithms.
+ Exploration vs. Exploitation: The agent needs to balance exploiting its current knowledge (using the actions with the highest posterior probabilities) with exploring new actions to improve its understanding of the environment. Techniques like Thompson sampling or Upper Confidence Bound (UCB) can be used to manage this trade-off.

